import streamlit as st
from llama_cpp import Llama
import numpy as np
import requests
import os
from urllib.parse import urlencode
import zipfile
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.vectorstores.faiss import FAISS
import torch

# –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –ª–µ–π–∞—É—Ç–∞
st.set_page_config(page_title="Saiga", page_icon="üß†", layout="wide", )

# CSS —Å—Ç–∏–ª–∏
# def local_css(file_name):
#     with open(file_name) as f:
#         st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
#
# local_css("styles.css")


# Add elements to vertical setting menu
st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")

# Add video source selection dropdown
source = st.sidebar.selectbox(
    "–ú–æ–¥–µ–ª—å",
    ("Saiga",'Mistral-Nemo',"ruGPT-3.5-13B"),
)
if source == "Saiga":
    n_ctx = 8192 # –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
    model_path = "model-q4_K.gguf"
    model_url = "https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q4_K.gguf"
    SYSTEM_PROMPT = ("""–¢—ã ‚Äî –°–∞–π–≥–∞, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –Æ—Ä–∏—Å—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –¥–µ–ª–∞–µ—à—å —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é.""")

elif source == "Mistral-Nemo":
    n_ctx = 128000 # –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
    model_path = "Mistral-Nemo.gguf"
    model_url = "https://huggingface.co/second-state/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf"
    SYSTEM_PROMPT = ("""–¢—ã ‚Äî –Æ—Ä–∏—Å—Ç, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –Æ—Ä–∏—Å—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –¥–µ–ª–∞–µ—à—å —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é —Å–æ —Å—ã–ª–∫–∞–º–∏ –Ω–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–æ.""")

elif source == "ruGPT-3.5-13B":
    n_ctx = 2040 # –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
    model_path = "ruGPT-3.5-13B.gguf"
    model_url = "https://huggingface.co/oblivious/ruGPT-3.5-13B-GGUF/resolve/main/ruGPT-3.5-13B-Q8_0.gguf"
    SYSTEM_PROMPT = ("""–¢—ã ‚Äî –Æ—Ä–∏—Å—Ç, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –Æ—Ä–∏—Å—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –¥–µ–ª–∞–µ—à—å —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é.""")



# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
def download_model(model_url, save_path,text):

    with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ {text}..."):
        # Create a progress bar
        progress_bar = st.progress(0)

        # Stream the download and update progress bar
        response = requests.get(model_url, stream=True)

        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0

        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    # Calculate progress and ensure it is between 0 and 100
                    progress = (downloaded_size / total_size)
                    progress_bar.progress(progress)

        # Close the progress bar once done
        st.success("–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
if not os.path.exists(model_path):
    text = '–º–æ–¥–µ–ª–∏'
    download_model(model_url, model_path, text)

#–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å –∏–∑ HuggingFace –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ç–µ—Å–∫—Ç–æ–≤, –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –ª—é–±—É—é –º–æ–¥–µ–ª—å, –≤–∫–ª—é—á–∞—è GPT –∏–ª–∏ GiGaChat
model_name=model_name_hug = 'sentence-transformers/all-MiniLM-L6-v2' # —Ç—É—Ç –Ω—É–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å  sentence sintisimilariti
model_kwargs = {'device': 'cuda'}
embeddings = HuggingFaceEmbeddings(model_name=model_name_hug, model_kwargs = model_kwargs )

name_base = 'faiss_index.zip'

base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'
public_key = 'https://disk.yandex.ru/d/wvdi-x-Nkt_hfA'  # –°—é–¥–∞ –≤–ø–∏—Å—ã–≤–∞–µ—Ç–µ –≤–∞—à—É —Å—Å—ã–ª–∫—É

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ä–∞—Å—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
final_url = base_url + urlencode(dict(public_key=public_key))
response = requests.get(final_url)
download_url = response.json()['href']
name_faile = 'faiss_index.zip'
download_response = requests.get(download_url)
with open(name_faile, 'wb') as f:   # –ó–¥–µ—Å—å —É–∫–∞–∂–∏—Ç–µ –Ω—É–∂–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    f.write(download_response.content)
with zipfile.ZipFile(name_faile, 'r') as zip_ref:
    zip_ref.extractall()

# –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º—Å—è –∫ –ë–î
db = FAISS.load_local(
    '/home/dimk/langchain/Streamlit/faiss_index',
    embeddings,
    allow_dangerous_deserialization=True
)


# –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
temperature = st.sidebar.slider("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏", 0.0, 1.0, 0.5, 0.01)


#—Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞

def interact(text,
    model_path="./model-q4_K.gguf",
    n_ctx=4192,
    top_k=30,
    top_p=0.9,
    temperature=0.8,
    repeat_penalty=1.1
):

    model = Llama(
        model_path=model_path,
        n_ctx=n_ctx,
        n_gpu_layers=-1,
        n_parts=1,
        verbose=False,
    )



    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    user_message = text
    messages.append({"role": "user", "content": user_message})
    for part in model.create_chat_completion(
            messages,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repeat_penalty=repeat_penalty,
            stream=True,
        ):
            delta = part["choices"][0]["delta"]
            if "content" in delta:
                yield delta["content"]


#–ß–∞—Ç-–±–æ—Ç
st.title('üêµüîó –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ LLM')

# Initialize chat history and text state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "text" not in st.session_state:
    st.session_state.text = ""

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# React to user input
if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å?"):
    # Display user message in chat message container
    st.chat_message("user").markdown(prompt)
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Initialize response variable
    response = ""

    # Placeholder for assistant's response
    assistant_message = st.chat_message("assistant")
    response_placeholder = assistant_message.empty()

    # Generate assistant response
    text_base = db.similarity_search_with_score(prompt, k=3)
    text_base = '\n\n'.join([f'–í–æ–ø—Ä–æ—Å: {i[0].page_content}\n–û—Ç–≤–µ—Ç: {i[0].metadata["–û—Ç–≤–µ—Ç"]}' for i in text_base if i[1] < 0.2])
    if text_base:
        promp = f"""–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: "{prompt}"
        –ù–∞ –ø–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n{text_base}. 
        –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞–ø–∏—à–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ—Å—Ç—Ä–æ–µ–Ω –æ—Ç–≤–µ—Ç"""
    else:
        promp = f"""–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: "{prompt}".
        –ù–∞ –ø–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n{text_base}. 
        –í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞–ø–∏—à–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ—Å—Ç—Ä–æ–µ–Ω –æ—Ç–≤–µ—Ç"""
    print()
    print(promp)
    for resp in interact(promp, temperature=temperature, n_ctx=n_ctx):
        response += resp
        response_placeholder.markdown(response, unsafe_allow_html=True)
    if text_base:
        response += '<h3><u>*–ü—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –æ—Ç–≤–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π*</u></h3>'
        response_placeholder.markdown(response, unsafe_allow_html=True)
    else:
        response += '<h3><u>*–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–ª–∞ –±–µ–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫*</u></h3>'
        response_placeholder.markdown(response, unsafe_allow_html=True)
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})



